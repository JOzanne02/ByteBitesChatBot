import torch
from transformers import AutoModelForCasualLLM, AutoTokenizer

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. Initialize Model and Tokenizer
# microsoft/DialoGPT-large is the pre-trained model name
MODEL_NAME = "microsoft/DialoGPT-large"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

def generate_chef_response(user_input, chat_history_ids=None):
    """Generates a conversational response using DialoGPT."""
    
    # Encode the new user input, adding the eos_token
    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, 
                                          return_tensors='pt')

    # Append the new user input to the chat history
    bot_input_ids = new_user_input_ids
    if chat_history_ids is not None:
        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)

    # 2. Generate a response
    chat_history_ids = model.generate(
        bot_input_ids, 
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=3,  # Prevents repetitive phrases
        do_sample=True,          # Introduces variation in responses
        top_k=50, 
        top_p=0.95, 
        temperature=0.75
    )
    
    # Decode the last generated response only
    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], 
                                skip_special_tokens=True)
    
    return response, chat_history_ids

# 3. Main Chat Loop
if __name__ == "__main__":
    print("ChefBot initialized. Ask me anything culinary! (Type 'quit' to exit)")
    
    # Store chat history to maintain context
    chat_history_ids = None 
    
    while True:
        user_input = input("You: ")
        
        if user_input.lower() == 'quit':
            print("Goodbye, and happy cooking!")
            break
            
        response, chat_history_ids = generate_chef_response(user_input, chat_history_ids)
        print(f"ChefBot: {response}")
